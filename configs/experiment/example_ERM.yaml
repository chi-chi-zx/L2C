# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /data: iwild_standard_supervised.yaml
  - override /model: prompt_tta_supervised.yaml
  - override /callbacks: default.yaml
  - override /trainer: default.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

trainer:
#  devices: [2]
  max_epochs: 30
  max_steps: 200000 

# path to data directory
paths:
  data_dir: "/data/tao/wilds/data"

model:
  train_support_ratio: 0.10
  train_coef_prompt_loss: 0
  train_coef_corr_loss: 0
  optimizer:
    optimizer: sgd
    base_lr: 0.00001
    weight_decay: 0.01
    momentum: 0.9
 
  model:
    num_prompts: 100
    correlation_loss: True
  

  scheduler:
    # scheduler: hard_steps
    warmup_epoch: 0
    total_epoch: 30

data:   
    batch_size: 32
    # n_negative_groups_per_batch: 2                                                                                                 
    # n_points_per_negative_group: 2
    input_resolution: 224

task_name: "Fmow_Finetune"
test: False
seed: 42
