# Learning to Adapt Frozen CLIP for Few-Shot Test-Time Domain Adaptation (ICLR 2025)
[Paper](https://openreview.net/pdf?id=TD3SGJfBC7)

## ðŸ’¡ Abstract
Few-shot Test-Time Domain Adaptation focuses on adapting a model at test time to a specific domain using only a few unlabeled examples, addressing domain shift. Prior methods leverage CLIP's strong out-of-distribution (OOD) abilities by generating domain-specific prompts to guide its generalized, frozen features. However, since downstream datasets are not explicitly seen by CLIP, solely depending on the feature space knowledge is constrained by CLIP's prior knowledge. Notably, when using a less robust backbone like ViT-B/16, performance significantly drops on challenging real-world benchmarks. Departing from the state-of-the-art of inheriting the intrinsic OOD capability of CLIP, this work introduces learning directly on the input space to complement the dataset-specific knowledge for frozen CLIP. Specifically, an independent side branch is attached in parallel with CLIP and enforced to learn exclusive knowledge via revert attention.  To better capture the dataset-specific label semantics for downstream adaptation, we propose to enhance the inter-dispersion among text features via greedy text ensemble and refinement. The text and visual features are then progressively fused in a domain-aware manner by a generated domain prompt to adapt toward a specific domain. Extensive experiments show our method's superiority on 5 large-scale benchmarks (WILDS and DomainNet), notably improving over smaller networks like ViT-B/16 with gains of \textbf{+5.1} in F1 for iWildCam and \textbf{+3.1\%} in WC Acc for FMoW. 

## Installation
### Requirements
The code is tested on 
```bash
PyTorch = 2.1.2
CUDA = 12.2
pytorch-lightning = 2.0.0
openCLIP = 2.16
```

### Download datasets
**WILDS**

Please follow the download instructions provided by [WILDS benchmark](https://github.com/p-lambda/wilds/) to download iWildCam, Camelyon17, FMoW and ProvertyMap to the folder of `./data`

**DomainNet**

1. Download the official [DomainNet benchmark](http://ai.bu.edu/M3SDA/) to `./data`
2. Generate a metadata.csv for DomainNet by running the [WILDS preprocessing script](https://github.com/p-lambda/wilds/blob/472677590de351857197a9bf24958838c39c272b/dataset_preprocessing/domainnet/generate_metadata.py).

Noted, we evaluate the **`official test split`** of DomainNet instead of the random split of the target domain as in [DomainBed](https://github.com/facebookresearch/DomainBed) codebase.

## Project Structure
Reference: [lightning-hydra-template](https://github.com/ashleve/lightning-hydra-template/)

```
â”œâ”€â”€ configs                   <- Hydra configs
â”‚   â”œâ”€â”€ callbacks                <- Callbacks configs
â”‚   â”œâ”€â”€ data                     <- Data configs
â”‚   â”œâ”€â”€ experiment               <- Experiment configs
â”‚   â”œâ”€â”€ extras                   <- Extra utilities configs
â”‚   â”œâ”€â”€ logger                   <- Logger configs
â”‚   â”œâ”€â”€ model                    <- Model configs
â”‚   â”œâ”€â”€ paths                    <- Project paths configs
â”‚   â”œâ”€â”€ trainer                  <- Trainer configs
â”‚   â”œâ”€â”€ eval.yaml             <- Main config for evaluation
â”‚   â””â”€â”€ train.yaml            <- Main config for training
â”‚
â”œâ”€â”€ logs                   <- Logs generated by hydra and lightning loggers
â”œâ”€â”€ modelzoo               <- pretrained model weights
â”œâ”€â”€ data                   <- downloaded datasets
â”œâ”€â”€ src                    <- Source code
â”‚   â”œâ”€â”€ datasets                <- benchmark datasets
â”‚   â”œâ”€â”€ models                  <- model components
â”‚   â”œâ”€â”€ lightning               <- LightningModule, DataModule, Callbacks
â”‚   â”œâ”€â”€ solver                  <- losses, solvers, schedulers
â”‚   â”œâ”€â”€ utils                   <- Utility modules
â”œâ”€â”€ train.py                    <- Run training
â”œâ”€â”€ eval.py                     <- Run evaluation
â”‚
â”œâ”€â”€ .env                      <- Example of file for storing private environment variables
â”œâ”€â”€ .project-root             <- File for inferring the position of project root directory
â”œâ”€â”€ requirements.yml          <- File for installing pip environment
â””â”€â”€ README.md
```
<br>

## Evaluation
### Download pretrained checkpoints 
From [OneDrive](https://drive.google.com/drive/folders/1x2Z678utcJjYPCD0KLnquxb2n048qkv2?usp=sharing) and save in the folder of `./modelzoo`




## <a name="cite"/> :clipboard: Citation

If you use this code in your research, please consider citing our paper:
```
@inproceedings{chi2025learning,
  title={Learning to adapt frozen clip for few-shot test-time domain adaptation},
  author={Chi, Zhixiang and Gu, Li and Liu, Huan and Wang, Ziqiang and Wu, Yanan and Wang, Yang and Plataniotis, Konstantinos N},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025}
}

@inproceedings{chiadapting,
  title={Adapting to Distribution Shift by Visual Domain Prompt Generation},
  author={Chi, Zhixiang and Gu, Li and Zhong, Tao and Liu, Huan and YU, YUANHAO and Plataniotis, Konstantinos N and Wang, Yang},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@article{zhong2022meta,
  title={Meta-dmoe: Adapting to domain shift by meta-distillation from mixture-of-experts},
  author={Zhong, Tao and Chi, Zhixiang and Gu, Li and Wang, Yang and Yu, Yuanhao and Tang, Jin},
  journal={Advances in Neural Information Processing Systems},
  year={2022}
}
```
